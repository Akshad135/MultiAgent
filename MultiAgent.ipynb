{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv5w5xBIy+++12M4PFRsqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshad135/MultiAgent/blob/main/MultiAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figuring dependencies"
      ],
      "metadata": {
        "id": "aemCGAEr_6-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "pcdBjx0G_opp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-tavily \\\n",
        "  langchain-community \\\n",
        "  tavily-python \\\n",
        "  langgraph \\\n",
        "  --quiet"
      ],
      "metadata": {
        "id": "9UBzfZEM46et"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Statements"
      ],
      "metadata": {
        "id": "rnF18dhp_1H6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CteK8spC4oy_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Any, Dict, List, Optional, Mapping\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.llms.base import BaseLLM\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Literal\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up agents"
      ],
      "metadata": {
        "id": "o5-1aj9vACoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP"
      ],
      "metadata": {
        "id": "5QKHxn9sAIvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "callback_manager = CallbackManager(callbacks)"
      ],
      "metadata": {
        "id": "gCbhQSfe5MkS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENVs"
      ],
      "metadata": {
        "id": "ZvTdpxGZAKTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_RETRIES = 3\n",
        "\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\""
      ],
      "metadata": {
        "id": "eFWZ1Lun4rSV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tavily setup"
      ],
      "metadata": {
        "id": "D5rIWnmlAMty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_search = TavilySearchResults(k=5)\n",
        "research_tool = Tool(\n",
        "    name=\"tavily_search\",\n",
        "    description=\"Use for factual web search via Tavily\",\n",
        "    func=tavily_search.run,\n",
        ")"
      ],
      "metadata": {
        "id": "LjbPJ3PR5UPi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Groq Setup"
      ],
      "metadata": {
        "id": "FeTzrJAWAO_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroqLLM(LLM):\n",
        "    model_name: str = \"llama-3.1-8b-instant\"\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 1024\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
        "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens)\n",
        "        }\n",
        "\n",
        "        if stop:\n",
        "            data[\"stop\"] = stop\n",
        "\n",
        "        try:\n",
        "            response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens\n",
        "        }"
      ],
      "metadata": {
        "id": "R2Rdl2a04wih"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating instances"
      ],
      "metadata": {
        "id": "HFpZW-2SAREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "research_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "answer_drafting_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024\n",
        ")"
      ],
      "metadata": {
        "id": "LqgpI_l-4y7b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Definitions"
      ],
      "metadata": {
        "id": "Tur4mk58AV7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGraphState(TypedDict):\n",
        "    user_query: str\n",
        "    snippets: list[dict]\n",
        "    verdict: str\n",
        "\n",
        "class DraftState(MyGraphState, total=False):\n",
        "    retry_count: int\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "ljSJsuk-5w2b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Prompts"
      ],
      "metadata": {
        "id": "XMTa1sOpAZXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_prompt = \"\"\"\n",
        "You are a Research Assistant.\n",
        "Call the function 'tavily_search' with this question: {query}\n",
        "\"\"\"\n",
        "\n",
        "evaluate_prompt = \"\"\"\n",
        "You are a Quality Evaluator.\n",
        "Given these snippets for the question: {query}\n",
        "\n",
        "Snippets:\n",
        "{snippets}\n",
        "\n",
        "Your task is to determine if ANY of these snippets contain relevant information that could help answer the question.\n",
        "Respond with ONLY 'yes' if ANY snippet contains relevant information.\n",
        "Respond with ONLY 'no' if NONE of the snippets contain relevant information.\n",
        "\"\"\"\n",
        "\n",
        "draft_prompt = \"\"\"\n",
        "You are an Answer Drafting Agent.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "INFORMATION SNIPPETS:\n",
        "{snippets}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Use ONLY the information in the snippets above to answer the query\n",
        "2. Write a comprehensive answer in paragraph form and not in point wise form.\n",
        "3. Include the most important facts and recent breakthroughs mentioned in the snippets\n",
        "4. Keep your answer factual and concise\n",
        "5. Do NOT include any JSON formatting in your answer\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ivKEvJhi5xpK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "YCHtdcD9AdMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_prompt_template = PromptTemplate.from_template(fetch_prompt)\n",
        "evaluate_prompt_template = PromptTemplate.from_template(evaluate_prompt)\n",
        "draft_prompt_template = PromptTemplate.from_template(draft_prompt)"
      ],
      "metadata": {
        "id": "XcNHg6wh5B9i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Chains"
      ],
      "metadata": {
        "id": "aY3le6uqAgPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_chain = fetch_prompt_template | research_llm\n",
        "evaluate_chain = evaluate_prompt_template | research_llm\n",
        "draft_chain = draft_prompt_template | answer_drafting_llm"
      ],
      "metadata": {
        "id": "Af32kQox-BMD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Architecture"
      ],
      "metadata": {
        "id": "j11uldxRAiSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl Web using Tavily"
      ],
      "metadata": {
        "id": "UGCzPH72AnUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_snippets(state: DraftState) -> DraftState:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "    print(f\"[FETCH] Searching for: '{state['user_query']}' (attempt {rc+1}/{MAX_RETRIES+1})\")\n",
        "\n",
        "    try:\n",
        "        raw = research_tool.run(state[\"user_query\"])\n",
        "        snippets = []\n",
        "\n",
        "        if isinstance(raw, str):\n",
        "            try:\n",
        "                data = json.loads(raw)\n",
        "            except json.JSONDecodeError:\n",
        "                data = [{\"text\": raw}]\n",
        "        else:\n",
        "            data = raw\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            snippet_list = [data]\n",
        "        elif isinstance(data, list):\n",
        "            snippet_list = data\n",
        "        else:\n",
        "            snippet_list = [{\"text\": str(data)}]\n",
        "\n",
        "        for idx, item in enumerate(snippet_list):\n",
        "            if isinstance(item, dict):\n",
        "                text = None\n",
        "                for key in [\"content\", \"text\", \"snippet\"]:\n",
        "                    if key in item and item[key]:\n",
        "                        text = item[key]\n",
        "                        break\n",
        "\n",
        "                if not text:\n",
        "                    text = str(item)\n",
        "\n",
        "                if isinstance(text, str) and text.startswith(\"{\") and \"content\" in text:\n",
        "                    try:\n",
        "                        parsed = json.loads(text.replace(\"'\", \"\\\"\"))\n",
        "                        if \"content\" in parsed:\n",
        "                            text = parsed[\"content\"]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                snippets.append({\"id\": idx, \"text\": text})\n",
        "            else:\n",
        "                snippets.append({\"id\": idx, \"text\": str(item)})\n",
        "\n",
        "        print(f\"[FETCH] Found {len(snippets)} snippets\")\n",
        "        if snippets:\n",
        "            preview = snippets[0][\"text\"][:150] + \"...\" if len(snippets[0][\"text\"]) > 150 else snippets[0][\"text\"]\n",
        "            print(f\"[FETCH] Sample: \\\"{preview}\\\"\")\n",
        "\n",
        "        verdict = \"yes\" if snippets else \"no\"\n",
        "\n",
        "    except Exception as e:\n",
        "        snippets = []\n",
        "        verdict = \"no\"\n",
        "        print(f\"[FETCH] Error occurred during search\")\n",
        "\n",
        "    return {\n",
        "        \"user_query\": state[\"user_query\"],\n",
        "        \"snippets\": snippets,\n",
        "        \"verdict\": verdict,\n",
        "        \"retry_count\": rc\n",
        "    }"
      ],
      "metadata": {
        "id": "Z2vJXEU55EW5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the fetch results"
      ],
      "metadata": {
        "id": "Ym1VjnPQAuB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[EVALUATE] Analyzing {len(state['snippets'])} snippets for relevance\")\n",
        "\n",
        "    if not state[\"snippets\"] or len(state[\"snippets\"]) == 0:\n",
        "        return {**state, \"verdict\": \"no\"}\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in state[\"snippets\"]:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']+1}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = evaluate_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        verdict = research_llm._call(formatted_prompt).strip().lower()\n",
        "\n",
        "        if \"yes\" in verdict:\n",
        "            verdict = \"yes\"\n",
        "        else:\n",
        "            verdict = \"no\"\n",
        "\n",
        "    except Exception as e:\n",
        "        verdict = state[\"verdict\"]\n",
        "\n",
        "    print(f\"[EVALUATE] Verdict: {verdict.upper()}\")\n",
        "    return {**state, \"verdict\": verdict}"
      ],
      "metadata": {
        "id": "4gEUq2j46vs6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide the next steps"
      ],
      "metadata": {
        "id": "IbQkv0wqAwXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_next_step(state: DraftState) -> Literal[\"fetch_snippets\", \"ingest_snippets\"]:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "\n",
        "    if state[\"verdict\"].lower().startswith(\"no\") and rc < MAX_RETRIES:\n",
        "        print(f\"[DECIDE] Insufficient information, retrying search\")\n",
        "        return \"fetch_snippets\"\n",
        "\n",
        "    if state[\"verdict\"].lower().startswith(\"no\"):\n",
        "        print(f\"[DECIDE] Max retries reached, proceeding with available data\")\n",
        "\n",
        "    print(f\"[DECIDE] Sufficient information found, proceeding to draft\")\n",
        "    return \"ingest_snippets\""
      ],
      "metadata": {
        "id": "ZxCZOEX56xdw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest the snippets"
      ],
      "metadata": {
        "id": "Xe6Ns-jgAyOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[INGEST] Processing {len(state['snippets'])} snippets for drafting\")\n",
        "    return {**state, \"retry_count\": state.get(\"retry_count\", 0) + 1}"
      ],
      "metadata": {
        "id": "7zuwhrc96zR_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft final answer"
      ],
      "metadata": {
        "id": "4FDNsOXGA3pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draft_answer(state: DraftState) -> DraftState:\n",
        "    print(f\"[DRAFT] Generating comprehensive response\")\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in state[\"snippets\"]:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']+1}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = draft_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        answer = answer_drafting_llm._call(formatted_prompt)\n",
        "\n",
        "    except Exception as e:\n",
        "        answer = f\"Unable to generate answer due to technical difficulties.\"\n",
        "\n",
        "    print(f\"[DRAFT] Response generated ({len(answer)} chars)\")\n",
        "    return {**state, \"answer\": answer}"
      ],
      "metadata": {
        "id": "SI0tEAih606b"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the graph"
      ],
      "metadata": {
        "id": "4akzm3Z0A520"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(DraftState)\n",
        "graph.add_node(\"fetch_snippets\", fetch_snippets)\n",
        "graph.add_node(\"evaluate_snippets\", evaluate_snippets)\n",
        "graph.add_node(\"decide_next_step\", decide_next_step)\n",
        "graph.add_node(\"ingest_snippets\", ingest_snippets)\n",
        "graph.add_node(\"draft_answer\", draft_answer)\n",
        "\n",
        "graph.set_entry_point(\"fetch_snippets\")\n",
        "graph.add_edge(\"fetch_snippets\", \"evaluate_snippets\")\n",
        "graph.add_conditional_edges(\n",
        "    \"evaluate_snippets\",\n",
        "    decide_next_step,\n",
        "    {\n",
        "        \"fetch_snippets\": \"fetch_snippets\",\n",
        "        \"ingest_snippets\": \"ingest_snippets\"\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"ingest_snippets\", \"draft_answer\")\n",
        "graph.add_edge(\"draft_answer\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "rhwKifov62oc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final answer"
      ],
      "metadata": {
        "id": "_27rr06gA9OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_research(query):\n",
        "    initial_state = {\"user_query\": query, \"retry_count\": 0}\n",
        "    print(f\"\\nRESEARCH QUERY: {query}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nFINAL ANSWER:\")\n",
        "    print(final_state.get(\"answer\", \"No answer was generated.\"))\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return final_state"
      ],
      "metadata": {
        "id": "1kT1IGa065hn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    result = run_research(\"Tell me about the book 'Tuesdays with Morrie'?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C32-h0Ec67lI",
        "outputId": "560b6ea5-e28d-481d-efcc-3f49802b80d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESEARCH QUERY: Tell me about the book 'Tuesdays with Morrie'?\n",
            "==================================================\n",
            "[FETCH] Searching for: 'Tell me about the book 'Tuesdays with Morrie'?' (attempt 1/4)\n",
            "[FETCH] Found 5 snippets\n",
            "[FETCH] Sample: \"Tuesdays with Morrie: An Old Man, A Young Man and Life's Greatest Lesson is a 1997 memoir by American author Mitch Albom. The book is about a series o...\"\n",
            "[EVALUATE] Analyzing 5 snippets for relevance\n",
            "[EVALUATE] Verdict: YES\n",
            "[DECIDE] Sufficient information found, proceeding to draft\n",
            "[INGEST] Processing 5 snippets for drafting\n",
            "[DRAFT] Generating comprehensive response\n",
            "[DRAFT] Response generated (1659 chars)\n",
            "==================================================\n",
            "\n",
            "FINAL ANSWER:\n",
            "'Tuesdays with Morrie: An Old Man, A Young Man and Life's Greatest Lesson' is a 1997 memoir by American author Mitch Albom. The book revolves around a series of visits Albom made to his former Brandeis University sociology professor, Morrie Schwartz, who was diagnosed with amyotrophic lateral sclerosis (ALS). As Morrie's condition progresses, he shares his wisdom and life lessons with Mitch, imparting valuable insights on love, forgiveness, and the importance of living.\n",
            "\n",
            "The book is a true story, with Mitch recounting his college days at Brandeis University in the spring of 1979, where he first met Morrie. Over the years, Mitch and Morrie reconnect, and Mitch visits his former professor every Tuesday, learning from his life experiences and observing how Morrie's progressive illness impacts his life. This personal and honest account is a transformative journey for Mitch, who gains new insights and perspectives on life.\n",
            "\n",
            "Throughout the book, Mitch shares Morrie's wisdom and teachings, as well as his own life story, highlighting the choices he made and the new understanding he gained during his time with Morrie. The disease and Morrie's physical decline paradoxically bring them closer together, creating a unique bond between the two men. The book is both an easy and thought-provoking read, overflowing with Morrie's life lessons and Mitch's reflections on their journey together.\n",
            "\n",
            "'Tuesdays with Morrie' has received critical attention, with features in prominent publications such as The Boston Globe and Nightline. The book's success can be attributed to its powerful and emotional storytelling, which has resonated with readers worldwide.\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}