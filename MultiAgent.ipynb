{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU85qsXMiUESMInd7yw584"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Figuring dependencies"
      ],
      "metadata": {
        "id": "aemCGAEr_6-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "pcdBjx0G_opp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-tavily \\\n",
        "  langchain-community \\\n",
        "  tavily-python \\\n",
        "  langgraph \\\n",
        "  faiss-cpu \\\n",
        "  --quiet"
      ],
      "metadata": {
        "id": "9UBzfZEM46et"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Statements"
      ],
      "metadata": {
        "id": "rnF18dhp_1H6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "CteK8spC4oy_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Any, Dict, List, Optional, Mapping, Tuple, Union\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.llms.base import BaseLLM\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Literal, Annotated\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up agents"
      ],
      "metadata": {
        "id": "o5-1aj9vACoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP"
      ],
      "metadata": {
        "id": "5QKHxn9sAIvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "callback_manager = CallbackManager(callbacks)"
      ],
      "metadata": {
        "id": "gCbhQSfe5MkS"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "jxhjGzy5FM3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_RETRIES = 3\n",
        "MAX_SNIPPETS_PER_SOURCE = 5\n",
        "RELEVANCE_THRESHOLD = 0.7"
      ],
      "metadata": {
        "id": "gpSjK-ZmFPTW"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENVs"
      ],
      "metadata": {
        "id": "ZvTdpxGZAKTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\""
      ],
      "metadata": {
        "id": "eFWZ1Lun4rSV"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "Wd6Ss6KLJ3B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleEmbeddings(Embeddings):\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [[hash(text) % 100 / 100 for _ in range(1536)] for text in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return [hash(text) % 100 / 100 for _ in range(1536)]\n",
        "\n",
        "embeddings = SimpleEmbeddings()"
      ],
      "metadata": {
        "id": "U2E9NRIpFT7U"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tavily setup"
      ],
      "metadata": {
        "id": "D5rIWnmlAMty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_search = TavilySearchResults(k=5)\n",
        "research_tool = Tool(\n",
        "    name=\"tavily_search\",\n",
        "    description=\"Use for factual web search via Tavily\",\n",
        "    func=tavily_search.run,\n",
        ")"
      ],
      "metadata": {
        "id": "LjbPJ3PR5UPi"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Groq Setup"
      ],
      "metadata": {
        "id": "FeTzrJAWAO_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroqLLM(LLM):\n",
        "    model_name: str = \"llama-3.1-8b-instant\"\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 1024\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
        "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens)\n",
        "        }\n",
        "\n",
        "        if stop:\n",
        "            data[\"stop\"] = stop\n",
        "\n",
        "        try:\n",
        "            response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens\n",
        "        }"
      ],
      "metadata": {
        "id": "R2Rdl2a04wih"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating instances"
      ],
      "metadata": {
        "id": "HFpZW-2SAREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "research_llm = GroqLLM(\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=4096,\n",
        ")\n",
        "\n",
        "answer_drafting_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=2048\n",
        ")\n",
        "\n",
        "fact_checking_llm = GroqLLM(\n",
        "    model_name=\"llama-guard-3-8b\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=512,\n",
        ")"
      ],
      "metadata": {
        "id": "LqgpI_l-4y7b"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sub-query setup"
      ],
      "metadata": {
        "id": "LTX-D5t3J50b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Citation(BaseModel):\n",
        "    claim: str = Field(description=\"A claim made in the draft answer\")\n",
        "    supported: bool = Field(description=\"Whether the claim is supported by the evidence\")\n",
        "    evidence: Optional[str] = Field(description=\"The snippet that supports or contradicts the claim\")\n",
        "    confidence: float = Field(description=\"Confidence score from 0-1\")\n",
        "\n",
        "class FactCheckResult(BaseModel):\n",
        "    checked_claims: List[Citation] = Field(description=\"Results of fact checking the draft answer\")\n",
        "    overall_accuracy: float = Field(description=\"Overall accuracy score from 0-1\")\n",
        "\n",
        "fact_check_parser = PydanticOutputParser(pydantic_object=FactCheckResult)"
      ],
      "metadata": {
        "id": "UetR_t7fFm1u"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Definitions"
      ],
      "metadata": {
        "id": "Tur4mk58AV7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGraphState(TypedDict):\n",
        "    user_query: str\n",
        "    snippets: List[Dict]\n",
        "    all_snippets: List[Dict]\n",
        "    verdict: str\n",
        "    vector_store: Optional[Any]\n",
        "    sources: List[Dict]\n",
        "\n",
        "class DraftState(MyGraphState, total=False):\n",
        "    retry_count: int\n",
        "    answer: str\n",
        "    draft_answer: str\n",
        "    fact_check_result: Dict\n",
        "    final_answer: str"
      ],
      "metadata": {
        "id": "ljSJsuk-5w2b"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Prompts"
      ],
      "metadata": {
        "id": "XMTa1sOpAZXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_prompt = \"\"\"\n",
        "You are a Research Assistant.\n",
        "Call the function 'tavily_search' with this question: {query}\n",
        "\"\"\"\n",
        "\n",
        "evaluate_prompt = \"\"\"\n",
        "You are a Quality Evaluator.\n",
        "Given these snippets for the question: {query}\n",
        "\n",
        "Snippets:\n",
        "{snippets}\n",
        "\n",
        "Your task is to determine if ANY of these snippets contain relevant information that could help answer the question.\n",
        "Respond with ONLY 'yes' if ANY snippet contains relevant information.\n",
        "Respond with ONLY 'no' if NONE of the snippets contain relevant information.\n",
        "\"\"\"\n",
        "\n",
        "draft_prompt = \"\"\"\n",
        "You are an Answer Drafting Agent.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "INFORMATION SNIPPETS:\n",
        "{snippets}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Use ONLY the information in the snippets above to answer the query\n",
        "2. Write a comprehensive answer in paragraph form and not in point wise form.\n",
        "3. Include the most important facts and recent breakthroughs mentioned in the snippets\n",
        "4. Keep your answer factual and concise\n",
        "5. Do NOT include any JSON formatting in your answer\n",
        "\"\"\"\n",
        "\n",
        "fact_check_prompt = \"\"\"\n",
        "You are a Fact Checking Agent.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "ORIGINAL SNIPPETS:\n",
        "{snippets}\n",
        "\n",
        "DRAFT ANSWER:\n",
        "{draft_answer}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze the draft answer and identify the main claims or statements\n",
        "2. For each claim, verify if it is supported by the snippets provided\n",
        "3. Format your response according to the following structure:\n",
        "{format_instructions}\n",
        "\n",
        "Focus on accuracy and provide specific evidence from the snippets that supports or contradicts each claim.\n",
        "\"\"\"\n",
        "\n",
        "final_answer_prompt = \"\"\"\n",
        "You are a Final Answer Compiler.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "DRAFT ANSWER:\n",
        "{draft_answer}\n",
        "\n",
        "FACT CHECK RESULTS:\n",
        "{fact_check_results}\n",
        "\n",
        "SOURCES:\n",
        "{sources}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the draft answer and the fact check results\n",
        "2. Create a final comprehensive answer that:\n",
        "   a. Is accurate and factual, removing or modifying any unsupported claims\n",
        "   b. Maintains a coherent and well-structured flow\n",
        "   c. Includes citations to the original sources where appropriate using [Source X] notation\n",
        "3. End the answer with a \"Sources:\" section that lists the sources used\n",
        "4. Make the answer helpful, comprehensive, and accurate\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ivKEvJhi5xpK"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "YCHtdcD9AdMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_prompt_template = PromptTemplate.from_template(fetch_prompt)\n",
        "evaluate_prompt_template = PromptTemplate.from_template(evaluate_prompt)\n",
        "draft_prompt_template = PromptTemplate.from_template(draft_prompt)\n",
        "\n",
        "fact_check_template = PromptTemplate(\n",
        "    template=fact_check_prompt,\n",
        "    input_variables=[\"query\", \"snippets\", \"draft_answer\"],\n",
        "    partial_variables={\"format_instructions\": fact_check_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "final_answer_template = PromptTemplate.from_template(final_answer_prompt)"
      ],
      "metadata": {
        "id": "XcNHg6wh5B9i"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Chains"
      ],
      "metadata": {
        "id": "aY3le6uqAgPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_chain = fetch_prompt_template | research_llm\n",
        "evaluate_chain = evaluate_prompt_template | research_llm\n",
        "draft_chain = draft_prompt_template | answer_drafting_llm\n",
        "fact_check_chain = fact_check_template | fact_checking_llm\n",
        "final_answer_chain = final_answer_template | answer_drafting_llm"
      ],
      "metadata": {
        "id": "Af32kQox-BMD"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Architecture"
      ],
      "metadata": {
        "id": "j11uldxRAiSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl Web using Tavily"
      ],
      "metadata": {
        "id": "UGCzPH72AnUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_snippets(state: DraftState) -> DraftState:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    print(f\"[FETCH] Searching for query: '{query}' (attempt {rc+1}/{MAX_RETRIES+1})\")\n",
        "\n",
        "    try:\n",
        "        raw = research_tool.run(query)\n",
        "        snippets = []\n",
        "\n",
        "        if isinstance(raw, str):\n",
        "            try:\n",
        "                data = json.loads(raw)\n",
        "            except json.JSONDecodeError:\n",
        "                data = [{\"text\": raw}]\n",
        "        else:\n",
        "            data = raw\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            snippet_list = [data]\n",
        "        elif isinstance(data, list):\n",
        "            snippet_list = data\n",
        "        else:\n",
        "            snippet_list = [{\"text\": str(data)}]\n",
        "\n",
        "        sources = []\n",
        "\n",
        "        for idx, item in enumerate(snippet_list):\n",
        "            if isinstance(item, dict):\n",
        "                text = None\n",
        "                title = item.get(\"title\", \"Unknown Source\")\n",
        "                url = item.get(\"url\", \"#\")\n",
        "\n",
        "                for key in [\"content\", \"text\", \"snippet\"]:\n",
        "                    if key in item and item[key]:\n",
        "                        text = item[key]\n",
        "                        break\n",
        "\n",
        "                if not text:\n",
        "                    text = str(item)\n",
        "\n",
        "                if isinstance(text, str) and text.startswith(\"{\") and \"content\" in text:\n",
        "                    try:\n",
        "                        parsed = json.loads(text.replace(\"'\", \"\\\"\"))\n",
        "                        if \"content\" in parsed:\n",
        "                            text = parsed[\"content\"]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                snippet_id = len(all_snippets) + len(snippets) + 1\n",
        "                snippets.append({\n",
        "                    \"id\": snippet_id,\n",
        "                    \"text\": text,\n",
        "                    \"source_id\": len(sources) + 1,\n",
        "                    \"query\": query\n",
        "                })\n",
        "\n",
        "                if {\"title\": title, \"url\": url} not in sources:\n",
        "                    sources.append({\"title\": title, \"url\": url})\n",
        "            else:\n",
        "                snippet_id = len(all_snippets) + len(snippets) + 1\n",
        "                snippets.append({\n",
        "                    \"id\": snippet_id,\n",
        "                    \"text\": str(item),\n",
        "                    \"source_id\": 0,\n",
        "                    \"query\": query\n",
        "                })\n",
        "\n",
        "        print(f\"[FETCH] Found {len(snippets)} snippets\")\n",
        "        if snippets:\n",
        "            preview = snippets[0][\"text\"][:100] + \"...\" if len(snippets[0][\"text\"]) > 100 else snippets[0][\"text\"]\n",
        "            print(f\"[FETCH] Sample: \\\"{preview}\\\"\")\n",
        "\n",
        "        verdict = \"yes\" if snippets else \"no\"\n",
        "\n",
        "        updated_all_snippets = all_snippets + snippets\n",
        "\n",
        "        return {\n",
        "            **state,\n",
        "            \"user_query\": state[\"user_query\"],\n",
        "            \"snippets\": snippets,\n",
        "            \"all_snippets\": updated_all_snippets,\n",
        "            \"sources\": state.get(\"sources\", []) + sources,\n",
        "            \"verdict\": verdict,\n",
        "            \"retry_count\": rc\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[FETCH] Error occurred during search\")\n",
        "        return {\n",
        "            **state,\n",
        "            \"user_query\": state[\"user_query\"],\n",
        "            \"snippets\": [],\n",
        "            \"all_snippets\": state.get(\"all_snippets\", []),\n",
        "            \"sources\": state.get(\"sources\", []),\n",
        "            \"verdict\": \"no\",\n",
        "            \"retry_count\": rc\n",
        "        }"
      ],
      "metadata": {
        "id": "Z2vJXEU55EW5"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the fetch results"
      ],
      "metadata": {
        "id": "Ym1VjnPQAuB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[EVALUATE] Analyzing {len(state['snippets'])} snippets for relevance\")\n",
        "\n",
        "    if not state[\"snippets\"] or len(state[\"snippets\"]) == 0:\n",
        "        return {**state, \"verdict\": \"no\"}\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in state[\"snippets\"]:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = evaluate_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        verdict = research_llm._call(formatted_prompt).strip().lower()\n",
        "\n",
        "        if \"yes\" in verdict:\n",
        "            verdict = \"yes\"\n",
        "        else:\n",
        "            verdict = \"no\"\n",
        "\n",
        "    except Exception as e:\n",
        "        verdict = state[\"verdict\"]\n",
        "\n",
        "    print(f\"[EVALUATE] Verdict: {verdict.upper()}\")\n",
        "    return {**state, \"verdict\": verdict}\n"
      ],
      "metadata": {
        "id": "4gEUq2j46vs6"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide the next steps"
      ],
      "metadata": {
        "id": "IbQkv0wqAwXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_next_step(state: DraftState) -> Literal[\"fetch_snippets\", \"ingest_snippets\"]:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "\n",
        "    if state[\"verdict\"].lower() == \"no\" and rc < MAX_RETRIES:\n",
        "        print(f\"[DECIDE] Insufficient information, retrying search\")\n",
        "        return \"fetch_snippets\"\n",
        "\n",
        "    if state[\"verdict\"].lower() == \"no\":\n",
        "        print(f\"[DECIDE] Max retries reached, proceeding with available data\")\n",
        "\n",
        "    print(f\"[DECIDE] Sufficient information found, proceeding to draft\")\n",
        "    return \"ingest_snippets\""
      ],
      "metadata": {
        "id": "ZxCZOEX56xdw"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest the snippets"
      ],
      "metadata": {
        "id": "Xe6Ns-jgAyOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[INGEST] Processing {len(state.get('all_snippets', []))} total snippets for drafting\")\n",
        "\n",
        "    docs = []\n",
        "    for snippet in state.get(\"all_snippets\", []):\n",
        "        metadata = {\n",
        "            \"id\": snippet[\"id\"],\n",
        "            \"source_id\": snippet.get(\"source_id\", 0),\n",
        "            \"query\": snippet.get(\"query\", state[\"user_query\"])\n",
        "        }\n",
        "        docs.append(Document(page_content=snippet[\"text\"], metadata=metadata))\n",
        "\n",
        "    try:\n",
        "        if not docs:\n",
        "            print(\"[INGEST] No documents to ingest\")\n",
        "            return {**state, \"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
        "\n",
        "        print(f\"[INGEST] Creating vector store with {len(docs)} documents\")\n",
        "        vector_store = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "        return {\n",
        "            **state,\n",
        "            \"retry_count\": state.get(\"retry_count\", 0) + 1,\n",
        "            \"vector_store\": vector_store\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"[INGEST] Error creating vector store\")\n",
        "        return {**state, \"retry_count\": state.get(\"retry_count\", 0) + 1}"
      ],
      "metadata": {
        "id": "7zuwhrc96zR_"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft final answer"
      ],
      "metadata": {
        "id": "4FDNsOXGA3pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draft_answer(state: DraftState) -> DraftState:\n",
        "    print(f\"[DRAFT] Generating comprehensive response\")\n",
        "\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in all_snippets:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        source_id = snippet.get(\"source_id\", 0)\n",
        "        source_text = f\"[Source {source_id}]\" if source_id > 0 else \"\"\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text} {source_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = draft_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        draft_answer = answer_drafting_llm._call(formatted_prompt)\n",
        "\n",
        "    except Exception as e:\n",
        "        draft_answer = f\"Unable to generate answer due to technical difficulties.\"\n",
        "\n",
        "    print(f\"[DRAFT] Draft response generated ({len(draft_answer)} chars)\")\n",
        "    return {**state, \"draft_answer\": draft_answer}"
      ],
      "metadata": {
        "id": "SI0tEAih606b"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fact_check(state: DraftState) -> DraftState:\n",
        "    print(f\"[FACT-CHECK] Verifying claims in the draft answer\")\n",
        "\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "\n",
        "    if not all_snippets or not state.get(\"draft_answer\"):\n",
        "        print(\"[FACT-CHECK] No snippets or draft to check, skipping verification\")\n",
        "        return {**state, \"fact_check_result\": {\"overall_accuracy\": 0.5}}\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in all_snippets:\n",
        "        snippet_text = snippet[\"text\"][:300] + \"...\" if len(snippet[\"text\"]) > 300 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = fact_check_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input,\n",
        "            draft_answer=state[\"draft_answer\"]\n",
        "        )\n",
        "\n",
        "        fact_check_response = fact_checking_llm._call(formatted_prompt)\n",
        "\n",
        "        try:\n",
        "            import re\n",
        "            json_match = re.search(r'```json\\n(.*?)\\n```', fact_check_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "                fact_check_result = json.loads(json_str)\n",
        "            else:\n",
        "                fact_check_result = json.loads(fact_check_response)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"[FACT-CHECK] Failed to parse result, using default\")\n",
        "            fact_check_result = {\n",
        "                \"checked_claims\": [],\n",
        "                \"overall_accuracy\": 0.7\n",
        "            }\n",
        "\n",
        "        print(f\"[FACT-CHECK] Verification complete with accuracy: {fact_check_result.get('overall_accuracy', 0.7)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[FACT-CHECK] Error during verification\")\n",
        "        fact_check_result = {\n",
        "            \"checked_claims\": [],\n",
        "            \"overall_accuracy\": 0.5\n",
        "        }\n",
        "\n",
        "    return {**state, \"fact_check_result\": fact_check_result}"
      ],
      "metadata": {
        "id": "UBLavGIFGEkt"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Any, Dict, List, Optional, Mapping, Tuple, Union\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.llms.base import BaseLLM\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Literal, Annotated\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup\n",
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "callback_manager = CallbackManager(callbacks)\n",
        "\n",
        "# Constants\n",
        "MAX_RETRIES = 3\n",
        "MAX_SNIPPETS_PER_SOURCE = 5\n",
        "RELEVANCE_THRESHOLD = 0.7\n",
        "\n",
        "# ENVs\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Embeddings\n",
        "class SimpleEmbeddings(Embeddings):\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [[hash(text) % 100 / 100 for _ in range(1536)] for text in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return [hash(text) % 100 / 100 for _ in range(1536)]\n",
        "\n",
        "embeddings = SimpleEmbeddings()\n",
        "\n",
        "# Tavily setup\n",
        "tavily_search = TavilySearchResults(k=5)\n",
        "research_tool = Tool(\n",
        "    name=\"tavily_search\",\n",
        "    description=\"Use for factual web search via Tavily\",\n",
        "    func=tavily_search.run,\n",
        ")\n",
        "\n",
        "# Groq Setup\n",
        "class GroqLLM(LLM):\n",
        "    model_name: str = \"llama-3.1-8b-instant\"\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 1024\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
        "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens)\n",
        "        }\n",
        "\n",
        "        if stop:\n",
        "            data[\"stop\"] = stop\n",
        "\n",
        "        try:\n",
        "            response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens\n",
        "        }\n",
        "\n",
        "# Creating LLM instances\n",
        "research_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "answer_drafting_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "fact_checking_llm = GroqLLM(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "# Fact check models\n",
        "class Citation(BaseModel):\n",
        "    claim: str = Field(description=\"A claim made in the draft answer\")\n",
        "    supported: bool = Field(description=\"Whether the claim is supported by the evidence\")\n",
        "    evidence: Optional[str] = Field(description=\"The snippet that supports or contradicts the claim\")\n",
        "    confidence: float = Field(description=\"Confidence score from 0-1\")\n",
        "\n",
        "class FactCheckResult(BaseModel):\n",
        "    checked_claims: List[Citation] = Field(description=\"Results of fact checking the draft answer\")\n",
        "    overall_accuracy: float = Field(description=\"Overall accuracy score from 0-1\")\n",
        "\n",
        "fact_check_parser = PydanticOutputParser(pydantic_object=FactCheckResult)\n",
        "\n",
        "# State Definitions\n",
        "class MyGraphState(TypedDict):\n",
        "    user_query: str\n",
        "    snippets: List[Dict]\n",
        "    all_snippets: List[Dict]\n",
        "    verdict: str\n",
        "    vector_store: Optional[Any]\n",
        "    sources: List[Dict]\n",
        "\n",
        "class DraftState(MyGraphState, total=False):\n",
        "    retry_count: int\n",
        "    answer: str\n",
        "    draft_answer: str\n",
        "    fact_check_result: Dict\n",
        "    final_answer: str\n",
        "\n",
        "# Prompts\n",
        "fetch_prompt = \"\"\"\n",
        "You are a Research Assistant.\n",
        "Call the function 'tavily_search' with this question: {query}\n",
        "\"\"\"\n",
        "\n",
        "evaluate_prompt = \"\"\"\n",
        "You are a Quality Evaluator.\n",
        "Given these snippets for the question: {query}\n",
        "\n",
        "Snippets:\n",
        "{snippets}\n",
        "\n",
        "Your task is to determine if ANY of these snippets contain relevant information that could help answer the question.\n",
        "Respond with ONLY 'yes' if ANY snippet contains relevant information.\n",
        "Respond with ONLY 'no' if NONE of the snippets contain relevant information.\n",
        "\"\"\"\n",
        "\n",
        "draft_prompt = \"\"\"\n",
        "You are an Answer Drafting Agent.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "INFORMATION SNIPPETS:\n",
        "{snippets}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Use ONLY the information in the snippets above to answer the query\n",
        "2. Write a comprehensive answer in paragraph form and not in point wise form.\n",
        "3. Include the most important facts and recent breakthroughs mentioned in the snippets\n",
        "4. Keep your answer factual and concise\n",
        "5. Do NOT include any JSON formatting in your answer\n",
        "\"\"\"\n",
        "\n",
        "fact_check_prompt = \"\"\"\n",
        "You are a Fact Checking Agent.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "ORIGINAL SNIPPETS:\n",
        "{snippets}\n",
        "\n",
        "DRAFT ANSWER:\n",
        "{draft_answer}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Analyze the draft answer and identify the main claims or statements\n",
        "2. For each claim, verify if it is supported by the snippets provided\n",
        "3. Format your response according to the following structure:\n",
        "{format_instructions}\n",
        "\n",
        "Focus on accuracy and provide specific evidence from the snippets that supports or contradicts each claim.\n",
        "\"\"\"\n",
        "\n",
        "final_answer_prompt = \"\"\"\n",
        "You are a Final Answer Compiler.\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "DRAFT ANSWER:\n",
        "{draft_answer}\n",
        "\n",
        "FACT CHECK RESULTS:\n",
        "{fact_check_results}\n",
        "\n",
        "SOURCES:\n",
        "{sources}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Review the draft answer and the fact check results\n",
        "2. Create a final comprehensive answer that:\n",
        "   a. Is accurate and factual, removing or modifying any unsupported claims\n",
        "   b. Maintains a coherent and well-structured flow\n",
        "   c. Includes citations to the original sources where appropriate using [Source X] notation\n",
        "3. End the answer with a \"Sources:\" section that lists the sources used\n",
        "4. Make the answer helpful, comprehensive, and accurate\n",
        "\"\"\"\n",
        "\n",
        "# Prompt Templates\n",
        "fetch_prompt_template = PromptTemplate.from_template(fetch_prompt)\n",
        "evaluate_prompt_template = PromptTemplate.from_template(evaluate_prompt)\n",
        "draft_prompt_template = PromptTemplate.from_template(draft_prompt)\n",
        "\n",
        "fact_check_template = PromptTemplate(\n",
        "    template=fact_check_prompt,\n",
        "    input_variables=[\"query\", \"snippets\", \"draft_answer\"],\n",
        "    partial_variables={\"format_instructions\": fact_check_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "final_answer_template = PromptTemplate.from_template(final_answer_prompt)\n",
        "\n",
        "# Prompt Chains\n",
        "fetch_chain = fetch_prompt_template | research_llm\n",
        "evaluate_chain = evaluate_prompt_template | research_llm\n",
        "draft_chain = draft_prompt_template | answer_drafting_llm\n",
        "fact_check_chain = fact_check_template | fact_checking_llm\n",
        "final_answer_chain = final_answer_template | answer_drafting_llm\n",
        "\n",
        "# Agent Functions\n",
        "def fetch_snippets(state: DraftState) -> DraftState:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    print(f\"[FETCH] Searching for query: '{query}' (attempt {rc+1}/{MAX_RETRIES+1})\")\n",
        "\n",
        "    try:\n",
        "        raw = research_tool.run(query)\n",
        "        snippets = []\n",
        "\n",
        "        if isinstance(raw, str):\n",
        "            try:\n",
        "                data = json.loads(raw)\n",
        "            except json.JSONDecodeError:\n",
        "                data = [{\"text\": raw}]\n",
        "        else:\n",
        "            data = raw\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            snippet_list = [data]\n",
        "        elif isinstance(data, list):\n",
        "            snippet_list = data\n",
        "        else:\n",
        "            snippet_list = [{\"text\": str(data)}]\n",
        "\n",
        "        sources = []\n",
        "\n",
        "        for idx, item in enumerate(snippet_list):\n",
        "            if isinstance(item, dict):\n",
        "                text = None\n",
        "                title = item.get(\"title\", \"Unknown Source\")\n",
        "                url = item.get(\"url\", \"#\")\n",
        "\n",
        "                for key in [\"content\", \"text\", \"snippet\"]:\n",
        "                    if key in item and item[key]:\n",
        "                        text = item[key]\n",
        "                        break\n",
        "\n",
        "                if not text:\n",
        "                    text = str(item)\n",
        "\n",
        "                if isinstance(text, str) and text.startswith(\"{\") and \"content\" in text:\n",
        "                    try:\n",
        "                        parsed = json.loads(text.replace(\"'\", \"\\\"\"))\n",
        "                        if \"content\" in parsed:\n",
        "                            text = parsed[\"content\"]\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                snippet_id = len(all_snippets) + len(snippets) + 1\n",
        "                snippets.append({\n",
        "                    \"id\": snippet_id,\n",
        "                    \"text\": text,\n",
        "                    \"source_id\": len(sources) + 1,\n",
        "                    \"query\": query\n",
        "                })\n",
        "\n",
        "                if {\"title\": title, \"url\": url} not in sources:\n",
        "                    sources.append({\"title\": title, \"url\": url})\n",
        "            else:\n",
        "                snippet_id = len(all_snippets) + len(snippets) + 1\n",
        "                snippets.append({\n",
        "                    \"id\": snippet_id,\n",
        "                    \"text\": str(item),\n",
        "                    \"source_id\": 0,\n",
        "                    \"query\": query\n",
        "                })\n",
        "\n",
        "        print(f\"[FETCH] Found {len(snippets)} snippets\")\n",
        "        if snippets:\n",
        "            preview = snippets[0][\"text\"][:100] + \"...\" if len(snippets[0][\"text\"]) > 100 else snippets[0][\"text\"]\n",
        "            print(f\"[FETCH] Sample: \\\"{preview}\\\"\")\n",
        "\n",
        "        verdict = \"yes\" if snippets else \"no\"\n",
        "\n",
        "        updated_all_snippets = all_snippets + snippets\n",
        "\n",
        "        return {\n",
        "            **state,\n",
        "            \"user_query\": state[\"user_query\"],\n",
        "            \"snippets\": snippets,\n",
        "            \"all_snippets\": updated_all_snippets,\n",
        "            \"sources\": state.get(\"sources\", []) + sources,\n",
        "            \"verdict\": verdict,\n",
        "            \"retry_count\": rc\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[FETCH] Error occurred during search\")\n",
        "        return {\n",
        "            **state,\n",
        "            \"user_query\": state[\"user_query\"],\n",
        "            \"snippets\": [],\n",
        "            \"all_snippets\": state.get(\"all_snippets\", []),\n",
        "            \"sources\": state.get(\"sources\", []),\n",
        "            \"verdict\": \"no\",\n",
        "            \"retry_count\": rc\n",
        "        }\n",
        "\n",
        "def evaluate_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[EVALUATE] Analyzing {len(state['snippets'])} snippets for relevance\")\n",
        "\n",
        "    if not state[\"snippets\"] or len(state[\"snippets\"]) == 0:\n",
        "        return {**state, \"verdict\": \"no\"}\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in state[\"snippets\"]:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = evaluate_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        verdict = research_llm._call(formatted_prompt).strip().lower()\n",
        "\n",
        "        if \"yes\" in verdict:\n",
        "            verdict = \"yes\"\n",
        "        else:\n",
        "            verdict = \"no\"\n",
        "\n",
        "    except Exception as e:\n",
        "        verdict = state[\"verdict\"]\n",
        "\n",
        "    print(f\"[EVALUATE] Verdict: {verdict.upper()}\")\n",
        "    return {**state, \"verdict\": verdict}\n",
        "\n",
        "def decide_next_step(state: DraftState) -> Literal[\"fetch_snippets\", \"ingest_snippets\"]:\n",
        "    rc = state.get(\"retry_count\", 0)\n",
        "\n",
        "    if state[\"verdict\"].lower() == \"no\" and rc < MAX_RETRIES:\n",
        "        print(f\"[DECIDE] Insufficient information, retrying search\")\n",
        "        return \"fetch_snippets\"\n",
        "\n",
        "    if state[\"verdict\"].lower() == \"no\":\n",
        "        print(f\"[DECIDE] Max retries reached, proceeding with available data\")\n",
        "\n",
        "    print(f\"[DECIDE] Sufficient information found, proceeding to draft\")\n",
        "    return \"ingest_snippets\"\n",
        "\n",
        "def ingest_snippets(state: DraftState) -> DraftState:\n",
        "    print(f\"[INGEST] Processing {len(state.get('all_snippets', []))} total snippets for drafting\")\n",
        "\n",
        "    docs = []\n",
        "    for snippet in state.get(\"all_snippets\", []):\n",
        "        metadata = {\n",
        "            \"id\": snippet[\"id\"],\n",
        "            \"source_id\": snippet.get(\"source_id\", 0),\n",
        "            \"query\": snippet.get(\"query\", state[\"user_query\"])\n",
        "        }\n",
        "        docs.append(Document(page_content=snippet[\"text\"], metadata=metadata))\n",
        "\n",
        "    try:\n",
        "        if not docs:\n",
        "            print(\"[INGEST] No documents to ingest\")\n",
        "            return {**state, \"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
        "\n",
        "        print(f\"[INGEST] Creating vector store with {len(docs)} documents\")\n",
        "        vector_store = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "        return {\n",
        "            **state,\n",
        "            \"retry_count\": state.get(\"retry_count\", 0) + 1,\n",
        "            \"vector_store\": vector_store\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"[INGEST] Error creating vector store\")\n",
        "        return {**state, \"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
        "\n",
        "def draft_answer(state: DraftState) -> DraftState:\n",
        "    print(f\"[DRAFT] Generating comprehensive response\")\n",
        "\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in all_snippets:\n",
        "        snippet_text = snippet[\"text\"][:500] + \"...\" if len(snippet[\"text\"]) > 500 else snippet[\"text\"]\n",
        "        source_id = snippet.get(\"source_id\", 0)\n",
        "        source_text = f\"[Source {source_id}]\" if source_id > 0 else \"\"\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text} {source_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = draft_prompt_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input\n",
        "        )\n",
        "\n",
        "        draft_answer = answer_drafting_llm._call(formatted_prompt)\n",
        "\n",
        "    except Exception as e:\n",
        "        draft_answer = f\"Unable to generate answer due to technical difficulties.\"\n",
        "\n",
        "    print(f\"[DRAFT] Draft response generated ({len(draft_answer)} chars)\")\n",
        "    return {**state, \"draft_answer\": draft_answer}\n",
        "\n",
        "def fact_check(state: DraftState) -> DraftState:\n",
        "    print(f\"[FACT-CHECK] Verifying claims in the draft answer\")\n",
        "\n",
        "    all_snippets = state.get(\"all_snippets\", [])\n",
        "\n",
        "    if not all_snippets or not state.get(\"draft_answer\"):\n",
        "        print(\"[FACT-CHECK] No snippets or draft to check, skipping verification\")\n",
        "        return {**state, \"fact_check_result\": {\"overall_accuracy\": 0.5}}\n",
        "\n",
        "    formatted_snippets = []\n",
        "    for snippet in all_snippets:\n",
        "        snippet_text = snippet[\"text\"][:300] + \"...\" if len(snippet[\"text\"]) > 300 else snippet[\"text\"]\n",
        "        formatted_snippets.append(f\"Snippet {snippet['id']}: {snippet_text}\")\n",
        "\n",
        "    snippet_input = \"\\n\\n\".join(formatted_snippets)\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = fact_check_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            snippets=snippet_input,\n",
        "            draft_answer=state[\"draft_answer\"]\n",
        "        )\n",
        "\n",
        "        fact_check_response = fact_checking_llm._call(formatted_prompt)\n",
        "\n",
        "        try:\n",
        "            import re\n",
        "            json_match = re.search(r'```json\\n(.*?)\\n```', fact_check_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "                fact_check_result = json.loads(json_str)\n",
        "            else:\n",
        "                fact_check_result = json.loads(fact_check_response)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"[FACT-CHECK] Failed to parse result, using default\")\n",
        "            fact_check_result = {\n",
        "                \"checked_claims\": [],\n",
        "                \"overall_accuracy\": 0.7\n",
        "            }\n",
        "\n",
        "        print(f\"[FACT-CHECK] Verification complete with accuracy: {fact_check_result.get('overall_accuracy', 0.7)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[FACT-CHECK] Error during verification\")\n",
        "        fact_check_result = {\n",
        "            \"checked_claims\": [],\n",
        "            \"overall_accuracy\": 0.5\n",
        "        }\n",
        "\n",
        "    return {**state, \"fact_check_result\": fact_check_result}\n",
        "\n",
        "def compile_final_answer(state: DraftState) -> DraftState:\n",
        "    print(f\"[COMPILE] Creating final answer with citations\")\n",
        "\n",
        "    draft_answer = state.get(\"draft_answer\", \"No draft answer was generated.\")\n",
        "    fact_check_result = state.get(\"fact_check_result\", {\"overall_accuracy\": 0.5})\n",
        "    sources = state.get(\"sources\", [])\n",
        "\n",
        "    formatted_sources = []\n",
        "    for idx, source in enumerate(sources):\n",
        "        title = source.get(\"title\", \"Unknown Source\")\n",
        "        url = source.get(\"url\", \"#\")\n",
        "        formatted_sources.append(f\"Source {idx+1}: {title} - {url}\")\n",
        "\n",
        "    sources_text = \"\\n\".join(formatted_sources)\n",
        "\n",
        "    try:\n",
        "        simplified_fact_check = {\n",
        "            \"overall_accuracy\": fact_check_result.get(\"overall_accuracy\", 0.5),\n",
        "            \"summary\": f\"Overall accuracy is {fact_check_result.get('overall_accuracy', 0.5)*100:.0f}%\"\n",
        "        }\n",
        "\n",
        "        formatted_prompt = final_answer_template.format(\n",
        "            query=state[\"user_query\"],\n",
        "            draft_answer=draft_answer,\n",
        "            fact_check_results=json.dumps(simplified_fact_check, indent=2),\n",
        "            sources=sources_text\n",
        "        )\n",
        "\n",
        "        final_answer = answer_drafting_llm._call(formatted_prompt)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[COMPILE] Error during finalization\")\n",
        "        final_answer = draft_answer\n",
        "\n",
        "    print(f\"[COMPILE] Final answer created ({len(final_answer)} chars)\")\n",
        "    return {**state, \"answer\": final_answer}"
      ],
      "metadata": {
        "id": "DKLjRebCGKJu"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the graph"
      ],
      "metadata": {
        "id": "4akzm3Z0A520"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(DraftState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"fetch_snippets\", fetch_snippets)\n",
        "graph.add_node(\"evaluate_snippets\", evaluate_snippets)\n",
        "graph.add_node(\"decide_next_step\", decide_next_step)\n",
        "graph.add_node(\"ingest_snippets\", ingest_snippets)\n",
        "graph.add_node(\"create_draft\", draft_answer)\n",
        "graph.add_node(\"fact_check\", fact_check)\n",
        "graph.add_node(\"compile_final_answer\", compile_final_answer)\n",
        "\n",
        "# Connect nodes\n",
        "graph.set_entry_point(\"fetch_snippets\")\n",
        "graph.add_edge(\"fetch_snippets\", \"evaluate_snippets\")\n",
        "graph.add_conditional_edges(\n",
        "    \"evaluate_snippets\",\n",
        "    decide_next_step,\n",
        "    {\n",
        "        \"fetch_snippets\": \"fetch_snippets\",\n",
        "        \"ingest_snippets\": \"ingest_snippets\"\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"ingest_snippets\", \"create_draft\")\n",
        "graph.add_edge(\"create_draft\", \"fact_check\")\n",
        "graph.add_edge(\"fact_check\", \"compile_final_answer\")\n",
        "graph.add_edge(\"compile_final_answer\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "rhwKifov62oc"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final answer"
      ],
      "metadata": {
        "id": "_27rr06gA9OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_research(query):\n",
        "    initial_state = {\"user_query\": query, \"retry_count\": 0}\n",
        "    print(f\"\\nRESEARCH QUERY: {query}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(final_state.get(\"answer\", \"No answer was generated.\"))\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "1kT1IGa065hn"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    result = run_research(\"What are the leading innovations in Quantum Computing. Tell some applications for quantum computing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C32-h0Ec67lI",
        "outputId": "54d2c4e3-ab1d-448d-c0cc-d5985191f88c"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESEARCH QUERY: What are the leading innovations in Quantum Computing. Tell some applications for quantum computing\n",
            "==================================================\n",
            "[FETCH] Searching for query: 'What are the leading innovations in Quantum Computing. Tell some applications for quantum computing' (attempt 1/4)\n",
            "[FETCH] Found 5 snippets\n",
            "[FETCH] Sample: \"Quantum computing is already shaking up the world of cybersecurity. With techniques like Quantum Key...\"\n",
            "[EVALUATE] Analyzing 5 snippets for relevance\n",
            "[EVALUATE] Verdict: YES\n",
            "[DECIDE] Sufficient information found, proceeding to draft\n",
            "[INGEST] Processing 5 total snippets for drafting\n",
            "[INGEST] Creating vector store with 5 documents\n",
            "[DRAFT] Generating comprehensive response\n",
            "[DRAFT] Draft response generated (2285 chars)\n",
            "[FACT-CHECK] Verifying claims in the draft answer\n",
            "[FACT-CHECK] Failed to parse result, using default\n",
            "[FACT-CHECK] Verification complete with accuracy: 0.7\n",
            "[COMPILE] Creating final answer with citations\n",
            "[COMPILE] Final answer created (3128 chars)\n",
            "==================================================\n",
            "Final Answer:\n",
            "\n",
            "FINAL ANSWER:\n",
            "\n",
            "Quantum computing is a rapidly advancing field that is poised to revolutionize various industries. One of the leading innovations in quantum computing is its application in cybersecurity. Techniques like Quantum Key Distribution, Secure Multi-Party Computation, and quantum-enhanced blockchain are enhancing digital protection and setting the stage for a new era of data security [Source 1]. Companies like Quantinuum are pushing the boundaries of innovation through their Quantum Origin program, which has applications in post-quantum cryptography, VPN protocols resistant to quantum attacks, and on-demand key creation using Quantum Origin Cloud [Source 2].\n",
            "\n",
            "The potential applications of quantum computing extend far beyond cybersecurity, with industries like energy and manufacturing benefiting from its power. ExxonMobil, for instance, is using quantum computing to develop next-generation energy and manufacturing technologies, with potential applications in optimizing power grids and discovering new materials to capture carbon more efficiently [Source 3]. Additionally, quantum computing may be able to simulate complex quantum systems, enabling breakthroughs in fields like chemistry and materials science.\n",
            "\n",
            "Companies like Google Quantum AI, IBM, Microsoft, and AWS are at the forefront of quantum computing innovation, with each player focusing on specific areas such as integrating quantum computing with machine learning, developing cloud-based quantum computing services, and creating quantum programming languages and qubit technologies [Source 4]. These advancements have the potential to revolutionize industries and push the boundaries of human knowledge.\n",
            "\n",
            "The applications of quantum computing are vast and varied, with foreseeable uses including artificial intelligence and machine learning, which could lead to significant breakthroughs in areas like medicine, finance, and climate modeling [Source 5]. With the rapid progress being made in quantum computing, it is likely that we will see significant innovations in the years to come, transforming the way we live and work.\n",
            "\n",
            "However, it is essential to note that quantum computing is a rapidly evolving field, and the accuracy of its applications and innovations may change over time. As such, it is crucial to stay up-to-date with the latest developments and advancements in the field.\n",
            "\n",
            "Sources:\n",
            "[Source 1] Top 8 Quantum Computing Applications Shaping the Tech Market - https://markovate.com/quantum-computing-applications/\n",
            "[Source 2] 5 Crucial Quantum Computing Applications & Examples - https://thequantuminsider.com/2023/05/24/quantum-computing-applications/\n",
            "[Source 3] 10 Leading Quantum Computing Companies at the Forefront - https://www.bluequbit.io/quantum-computing-companies\n",
            "[Source 4] Quantum Industry Explained: Applications, Innovations & Challenges - https://thequantuminsider.com/2024/02/05/quantum-industry-explained-applications-innovations-challenges/\n",
            "[Source 5] Explore 7 future potential quantum computing uses - TechTarget - https://www.techtarget.com/searchdatacenter/tip/Explore-future-potential-quantum-computing-uses\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}